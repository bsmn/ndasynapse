{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import synapseclient\n",
    "import pandas\n",
    "import requests\n",
    "import boto3\n",
    "import nda_aws_token_generator\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(\"main\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "#create console handler and set level to debug\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.DEBUG)\n",
    "\n",
    "# NDA Configuration\n",
    "REFERENCE_GUID = 'NDAR_INVRT663MBL'\n",
    "\n",
    "# This is an old genomics subject\n",
    "EXCLUDE_GENOMICS_SUBJECTS = ('92027', )\n",
    "# EXCLUDE_EXPERIMENTS = ('534', '535')\n",
    "EXCLUDE_EXPERIMENTS = ()\n",
    "\n",
    "metadata_columns = ['src_subject_id', 'experiment_id', 'subjectkey', 'sample_id_original', \n",
    "                    'sample_id_biorepository',\n",
    "                    'subject_sample_id_original', 'biorepository', 'subject_biorepository', 'sample_description',\n",
    "                    'species', 'site', 'sex', 'sample_amount', 'phenotype', 'comments_misc']\n",
    "\n",
    "nda_bucket_name = 'nda-bsmn'\n",
    "\n",
    "# Synapse configuration\n",
    "synapse_data_folder = 'syn7872188'\n",
    "synapse_data_folder_id = int(synapse_data_folder.replace('syn', ''))\n",
    "storage_location_id = '9209'\n",
    "\n",
    "content_type_dict = {'.gz': 'application/x-gzip', '.bam': 'application/octet-stream', \n",
    "                     '.zip': 'application/zip'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credential configuration for NDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "obj = s3.Object('kdaily-lambda-creds.sagebase.org', 'ndalogs_config.json')\n",
    "\n",
    "config = json.loads(obj.get()['Body'].read())\n",
    "\n",
    "ndaconfig = config['nda']\n",
    "\n",
    "tokengenerator = nda_aws_token_generator.NDATokenGenerator()\n",
    "mytoken = tokengenerator.generate_token(ndaconfig['username'],\n",
    "                                        ndaconfig['password'])\n",
    "\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=mytoken.access_key,\n",
    "    aws_secret_access_key=mytoken.secret_key,\n",
    "    aws_session_token=mytoken.session\n",
    ")\n",
    "\n",
    "s3_nda = session.resource(\"s3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Samples\n",
    "\n",
    "Use the NDA api to get the `genomics_sample03` records for this GUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://ndar.nih.gov/api/guid/{}/data?short_name=genomics_sample03\".format(REFERENCE_GUID), \n",
    "                 auth=requests.auth.HTTPBasicAuth(ndaconfig['username'], ndaconfig['password']),\n",
    "                 headers={'Accept': 'application/json'})\n",
    "\n",
    "guid_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get data files from samples. There are currently up to two files per row.\n",
    "\n",
    "tmp = [{col['name']: col['value'] for col in row['dataElement']} \n",
    "       for row in guid_data['age'][0]['dataStructureRow']]\n",
    "    \n",
    "samples = pandas.io.json.json_normalize(tmp)\n",
    "\n",
    "colnames_lower = map(lambda x: x.lower(), samples.columns.tolist())\n",
    "samples.columns = colnames_lower\n",
    "\n",
    "# exclude some experiments\n",
    "samples = samples[~samples.experiment_id.isin(EXCLUDE_EXPERIMENTS)]\n",
    "\n",
    "samples['species'] = samples.organism.replace(['Homo Sapiens'], ['Human'])\n",
    "\n",
    "samples1 = samples[['src_subject_id', 'experiment_id', 'subjectkey', 'sample_id_original', 'sample_id_biorepository',\n",
    "                    'organism', 'species', 'sample_amount', 'sample_unit', 'biorepository',\n",
    "                    'comments_misc', 'site', 'data_file1', 'data_file1_type']]\n",
    "\n",
    "samples1.rename(columns={'data_file1': 'data_file', 'data_file1_type': 'fileFormat'}, inplace=True)\n",
    "\n",
    "samples2 = samples[['src_subject_id', 'experiment_id', 'subjectkey', 'sample_id_original', 'sample_id_biorepository',\n",
    "                    'organism', 'species', 'sample_amount', 'sample_unit', 'biorepository', \n",
    "                    'comments_misc', 'site', 'data_file2', 'data_file2_type']]\n",
    "\n",
    "samples2.rename(columns={'data_file2': 'data_file', 'data_file2_type': 'fileFormat'}, inplace=True)\n",
    "\n",
    "samples3 = pandas.concat([samples1, samples2], ignore_index=True)\n",
    "samples3.filter(~samples3.data_file.isnull())\n",
    "samples3['fileFormat'].replace(['BAM', 'FASTQ'], ['bam', 'fastq'], inplace=True)\n",
    "\n",
    "# Remove initial slash to match what is in manifest file\n",
    "samples3.data_file = samples3['data_file'].apply(lambda value: value[1:] if not pandas.isnull(value) else value)\n",
    "\n",
    "# Remove stuff that isn't part of s3 path\n",
    "samples3.data_file = map(lambda x: str(x).replace(\"![CDATA[\", \"\").replace(\"]]>\", \"\"), samples3.data_file.tolist())\n",
    "\n",
    "samples3 = samples3[samples3.data_file != 'nan']\n",
    "\n",
    "samples3.to_csv(\"./samples3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Subjects\n",
    "\n",
    "Use the NDA api to get the `genomics_subject02` records for this GUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://ndar.nih.gov/api/guid/{}/data?short_name=genomics_subject02\".format(REFERENCE_GUID), \n",
    "                 auth=requests.auth.HTTPBasicAuth(ndaconfig['username'], ndaconfig['password']),\n",
    "                 headers={'Accept': 'application/json'})\n",
    "\n",
    "subject_guid_data = json.loads(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_subject = []\n",
    "for row in subject_guid_data['age'][0]['dataStructureRow']:\n",
    "    foo = {col['name']: col['value'] for col in row['dataElement']}\n",
    "    tmp_subject.append(foo)\n",
    "    \n",
    "subjects = pandas.io.json.json_normalize(tmp_subject)\n",
    "subjects = subjects[~subjects.GENOMICS_SUBJECT02_ID.isin(EXCLUDE_GENOMICS_SUBJECTS)]\n",
    "\n",
    "colnames_lower = map(lambda x: x.lower(), subjects.columns.tolist())\n",
    "subjects.columns = colnames_lower\n",
    "\n",
    "subjects = subjects.assign(sex=subjects.gender.replace(['M', 'F'], ['male', 'female']),\n",
    "                           subject_sample_id_original=subjects.sample_id_original,\n",
    "                           subject_biorepository=subjects.biorepository)\n",
    "\n",
    "subjects = subjects[['src_subject_id', 'subjectkey', 'gender', 'race', 'phenotype', \n",
    "                   'subject_sample_id_original', 'sample_description', 'subject_biorepository', 'sex']]\n",
    "\n",
    "subjects = subjects.drop_duplicates()\n",
    "\n",
    "subjects.to_csv(\"./subjects.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Tissues\n",
    "\n",
    "Use the NDA api to get the `ncihd_btb02` records for this GUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://ndar.nih.gov/api/guid/{}/data?short_name=nichd_btb02\".format(REFERENCE_GUID), \n",
    "                 auth=requests.auth.HTTPBasicAuth(ndaconfig['username'], ndaconfig['password']),\n",
    "                 headers={'Accept': 'application/json'})\n",
    "\n",
    "btb_guid_data = json.loads(r.text)\n",
    "\n",
    "tmp_btb = []\n",
    "for row in btb_guid_data['age'][0]['dataStructureRow']:\n",
    "    foo = {col['name']: col['value'] for col in row['dataElement']}\n",
    "    tmp_btb.append(foo)\n",
    "    \n",
    "btb = pandas.io.json.json_normalize(tmp_btb)\n",
    "\n",
    "colnames_lower = map(lambda x: x.lower(), btb.columns.tolist())\n",
    "btb.columns = colnames_lower\n",
    "\n",
    "btb.drop('nichd_btb02_id', axis=1, inplace=True)\n",
    "btb = btb.drop_duplicates()\n",
    "\n",
    "btb.to_csv('./btb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Manifests\n",
    "\n",
    "Get list of `.manifest` files from the NDA-BSMN bucket. Read them in and concatenate them, under the assumption that the files listed in the manifest are in the same directory as the manifest file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bucket = s3_nda.Bucket(nda_bucket_name)\n",
    "manifests = [x for x in bucket.objects.all() if x.key.find('.manifest') >=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "manifest = pandas.DataFrame()\n",
    "\n",
    "for m in manifests:\n",
    "    folder = os.path.split(m.key)[0]\n",
    "    tmp = pandas.read_csv(m.get()['Body'], delimiter=\"\\t\", header=None)\n",
    "    tmp.columns = ('filename', 'md5', 'size')\n",
    "    tmp.filename = \"s3://%s/%s/\" % (nda_bucket_name, folder,) + tmp.filename.map(str)\n",
    "    manifest = pandas.concat([manifest, tmp])\n",
    "\n",
    "manifest.reset_index(drop=True, inplace=True)\n",
    "\n",
    "manifest.to_csv('./manifest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge together the tissue file and the subjects file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "btb_subjects = btb.merge(subjects, how=\"left\",\n",
    "                         left_on=[\"src_subject_id\", \"subjectkey\", \"race\", \"gender\"],\n",
    "                         right_on=[\"src_subject_id\", \"subjectkey\", \"race\", \"gender\"])\n",
    "\n",
    "btb_subjects = btb_subjects.assign(sample_id_biorepository=btb_subjects.sample_id_original)\n",
    "\n",
    "# Drop this as it will come back from the samples\n",
    "btb_subjects.drop('sample_id_original', axis=1, inplace=True)\n",
    "\n",
    "btb_subjects.to_csv('btb_subjects.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the tissue/subject with the samples to make a complete metadata table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metadata = samples3.merge(btb_subjects, how=\"left\",\n",
    "                          left_on=[\"src_subject_id\", \"subjectkey\", \"sample_id_biorepository\"],\n",
    "                          right_on=[\"src_subject_id\", \"subjectkey\", \"sample_id_biorepository\"])\n",
    "\n",
    "metadata.index = metadata.data_file\n",
    "\n",
    "metadata = metadata.drop_duplicates()\n",
    "\n",
    "metadata = metadata[metadata_columns]\n",
    "\n",
    "metadata.to_csv(\"./metadata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synapse\n",
    "\n",
    "Using the concatenated manifests as the master list of files to store, create file handles and entities in Synapse.\n",
    "\n",
    "Use the metadata table to get the appropriate tissue/subject/sample annotations to set on each File entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syn = synapseclient.login(silent=True)\n",
    "\n",
    "update = True\n",
    "\n",
    "for n, x in manifest.iterrows():\n",
    "    s3Key = x['filename'].replace(\"s3://%s/\" % nda_bucket_name, \"\")\n",
    "    s3FilePath = os.path.split(s3Key)[-1]\n",
    "    contentSize = x['size']\n",
    "    contentMd5 = x['md5']\n",
    "    \n",
    "    logger.debug(\"%s - %s\" % (s3Key, s3FilePath))\n",
    "\n",
    "    # Check if it exists in Synapse\n",
    "    res = syn.restGET(\"/entity/md5/%s\" % (contentMd5, ))['results']\n",
    "    \n",
    "    res = filter(lambda x: x['benefactorId'] == synapse_data_folder_id, res)\n",
    "    \n",
    "    if len(res) > 0:\n",
    "        \n",
    "        fhs = [syn.restGET(\"/entity/%(id)s/version/%(versionNumber)s/filehandles\" % er) for er in res]    \n",
    "        fileHandleObj = syn._getFileHandle(fhs[0]['list'][0]['id'])\n",
    "\n",
    "#         logger.info(\"%s already exists in Synapse (count = %s). Reusing fh %s\" % (os.path.split(s3Key)[1], \n",
    "#                                                                                   len(res), fileHandleObj))\n",
    "            \n",
    "    else:\n",
    "        # print \"Adding %s (%s)\" % (contentMd5, x['filename'])\n",
    "\n",
    "        \n",
    "        contentType = content_type_dict.get(os.path.splitext(x['filename'])[-1],\n",
    "                                            'application/octet-stream')\n",
    "        \n",
    "        fileHandle = {'concreteType': 'org.sagebionetworks.repo.model.file.S3FileHandle',\n",
    "                      'fileName'    : s3FilePath,\n",
    "                      'contentSize' : contentSize,\n",
    "                      'contentType' : contentType,\n",
    "                      'contentMd5' :  contentMd5,\n",
    "                      'bucketName' : nda_bucket_name,\n",
    "                      'key'        : s3Key,\n",
    "                      'storageLocationId' : storage_location_id}\n",
    "\n",
    "        fileHandleObj = syn.restPOST('/externalFileHandle/s3', \n",
    "                                     json.dumps(fileHandle), \n",
    "                                     endpoint=syn.fileHandleEndpoint)\n",
    "\n",
    "    try:\n",
    "        a = metadata.loc[x['filename']].to_dict()\n",
    "        \n",
    "        logger.debug(\"filename = %s, annotations = %s\" % (x['filename'], a))\n",
    "\n",
    "        f = synapseclient.File(parentId=synapse_data_folder, \n",
    "                               name=s3FilePath, \n",
    "                               dataFileHandleId = fileHandleObj['id'])\n",
    "        f.annotations = a\n",
    "\n",
    "        f = syn.store(f, forceVersion=False)\n",
    "    except KeyError:\n",
    "        logger.debug(\"Error getting metadata to annotation dictionary %s\" % (x['filename'], ))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
